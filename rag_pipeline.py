import os
from pathlib import Path
import re

from langchain.chains import RetrievalQA, LLMChain
from langchain.prompts import PromptTemplate, ChatPromptTemplate

# from langchain.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI

# from langchain.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAIEmbeddings

# from langchain.vectorstores import FAISS
from langchain_community.vectorstores import FAISS

from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

from langchain.schema import Document

import spacy
import streamlit as st
import subprocess
import importlib.util

# ===== æ£€æŸ¥ spaCy æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å°±è‡ªåŠ¨ä¸‹è½½ ===== 
model_name = "en_core_web_sm"
if not importlib.util.find_spec(model_name):
    subprocess.run(["python", "-m", "spacy", "download", model_name])

nlp = spacy.load(model_name)

def spacy_tokenizer(text: str) -> list[str]:
    return [token.text for token in nlp(text) if not token.is_space]

# ===== è®¾ç½® OpenAI API Key =====
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# ===== åŠ è½½ spaCy åˆ†è¯å™¨ï¼ˆç”¨äº BM25ï¼‰=====
# nlp = spacy.load("en_core_web_sm")
# def spacy_tokenizer(text):
#     return [token.text for token in nlp(text)]

nlp = spacy.load("en_core_web_sm")
def spacy_tokenizer(text: str) -> list[str]:
    return [token.text for token in nlp(text) if not token.is_space]

# ===== åŠ è½½æ–‡æ¡£ =====
chunk_path = Path("data/handbook_chunks.txt")
with open(chunk_path, "r", encoding="utf-8") as f:
    content = f.read()

raw_chunks = content.split("\n\n")
docs = [Document(page_content=chunk) for chunk in raw_chunks if chunk.strip()]

bm25_retriever = BM25Retriever.from_documents(docs, tokenizer=spacy_tokenizer)

# ===== åˆå§‹åŒ–æ£€ç´¢å™¨ =====
bm25_retriever = BM25Retriever.from_documents(docs, tokenizer=spacy_tokenizer)
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
db = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization=True)
faiss_retriever = db.as_retriever(search_kwargs={"k": 10})

retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever],
    weights=[0.5, 0.5]
)

# ===== é—®é¢˜æ”¹å†™æ¨¡å— =====
rewrite_llm = ChatOpenAI(temperature=0, model_name="gpt-4o", max_tokens=4000)
query_rewrite_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are an assistant that reformulates user queries to improve document retrieval in a knowledge-based system. \
Your rewritten query should be specific, semantically rich, and suitable for searching in a vector database."),
    ("human", "Original query: {original_query}\n\nRewritten query:")
])
query_rewriter = query_rewrite_prompt | rewrite_llm

def rewrite_query(original_query: str) -> str:
    try:
        response = query_rewriter.invoke({"original_query": original_query})
        return response.content.strip() if response.content.strip() else original_query
    except:
        return original_query

# ===== å®šä¹‰ RAG Prompt =====
custom_prompt = PromptTemplate.from_template("""
You are an assistant that answers questions strictly based on the internal documentation.

Your first task is to extract and display **all relevant Process IDs** found in the context.
You must list the Process IDs explicitly before giving any answer.

Example:
Relevant Process IDs: 291, 295

Then provide a clear, concise, **logical** answer in the form of a workflow:
- Start by identifying the **first step** a person should take.
- Clearly describe each subsequent step, referencing the relevant Process ID.
- Use transition words such as "First", "Next", "Then".
- Briefly explain what each process does and why it matters.

--- 

Question: {question}
Context:
{context}

Answer:
""")

# ===== æ„å»º RAG é—®ç­”é“¾ =====
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True,
    chain_type="stuff",
    chain_type_kwargs={"prompt": custom_prompt},
    verbose=True,
)

# ===== ä¸»ç¨‹åºå…¥å£ =====
if __name__ == "__main__":
    while True:
        user_question = input("\nğŸ§  Please Input your question:(Input 'exit' to exit): \n> ")
        if user_question.lower() in ["exit", "quit"]:
            break

        rewritten = rewrite_query(user_question)
        print(f"\nğŸ” question after rewrting: {rewritten}")

        result = qa_chain.invoke({"query": rewritten})

        print("\nğŸ¤– AI Bot Answer: ")
        print(result["result"])

        print("\nğŸ“š Relevant Processes:")
        for doc in result["source_documents"]:
            lines = doc.page_content.strip().split("\n")
            title_line = next((line for line in lines if line.lower().startswith("process ")), None)
            if title_line:
                print(f"- {title_line.strip()}")
            else:
                # If no title line, show first line as fallback
                print(f"- {lines[0].strip()}")